{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pandas.plotting import scatter_matrix\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNS Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (16, 7)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(palette='pastel')\n",
    "sns.set_style({'axes.facecolor': '#dcdcdc', 'grid.color': '#f5f5f5'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Acquisition and Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the provided historical sales data for the electronics section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is already downloaded and provided as electornics.json. Let's load it into dataframe and print first five entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = pd.read_json('electronics.json')\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure the data includes customer demographics, purchase history, product details, spending amounts, and dates of transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check in data if data includes the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the columns showing that all required columns presents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and handle missing values using appropriate techniques like mean/median imputation or dropping rows/columns with excessive missingness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the missing values present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing dataframe, we can see that missing values represented by \"\". So, we replace it with np.nan for further procession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the data, I've found that there are some empty values and also some 'Hidden' values. Let's check there count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.replace('', np.nan).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.replace('Hidden', np.nan).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replace these values with np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.replace('', np.nan, inplace=True)\n",
    "sales_data.replace('Hidden', np.nan, inplace=True)\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's count the total missing value in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fill these missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.dropna(subset=['Customer_ID'], inplace=True) # Drop rows with missing values as this is the unique identifier for the data\n",
    "\n",
    "sales_data['Age'].fillna(sales_data['Age'].median(), inplace=True) # Replace Age missing values with median as age is numerical data\n",
    "\n",
    "sales_data['Gender'].fillna(sales_data['Gender'].mode()[0], inplace=True) # As Gender is categorical data, replacing its missing values with mode will be the best option\n",
    "sales_data['Income_Level'].fillna(sales_data['Income_Level'].mode()[0], inplace=True) # As Income Level is categorical, mode will be best for filling values\n",
    "\n",
    "# sales_data['Address'].fillna('Unknown', inplace=True)\n",
    "\n",
    "sales_data.drop(\"Address\", axis=1, inplace=True) # As address is not much important for our analysis, dropping the column will be the best option\n",
    "sales_data.drop(\"Transaction_ID\", axis=1, inplace=True) # As Transaction ID is not much important for our analysis, dropping the column will be the best option\n",
    "\n",
    "sales_data['Purchase_Date'].fillna(method='ffill', inplace=True) # Fill Purchase Date with the forward fill\n",
    "\n",
    "sales_data.dropna(subset=['Product_ID'], inplace=True) # Drop rows with missing values as this is the unique identifier to check the product which sold\n",
    "\n",
    "sales_data['Product_Category'].fillna(sales_data['Product_Category'].mode()[0], inplace=True)\n",
    "sales_data['Brand'].fillna(sales_data['Brand'].mode()[0], inplace=True)\n",
    "\n",
    "sales_data['Purchase_Amount'].fillna(sales_data['Purchase_Amount'].median(), inplace=True)\n",
    "sales_data['Average_Spending_Per_Purchase'].fillna(sales_data['Average_Spending_Per_Purchase'].median(), inplace=True)\n",
    "sales_data['Purchase_Frequency_Per_Month'].fillna(sales_data['Purchase_Frequency_Per_Month'].median(), inplace=True)\n",
    "sales_data['Brand_Affinity_Score'].fillna(sales_data['Brand_Affinity_Score'].median(), inplace=True)\n",
    "\n",
    "sales_data['Product_Category_Preferences'].fillna(sales_data['Product_Category_Preferences'].mode()[0], inplace=True)\n",
    "sales_data['Month'].fillna(sales_data['Month'].mode()[0], inplace=True)\n",
    "sales_data['Year'].fillna(sales_data['Year'].mode()[0], inplace=True)\n",
    "sales_data['Season'].fillna(sales_data['Season'].mode()[0], inplace=True)\n",
    "\n",
    "sales_data.reset_index(inplace=True)\n",
    "sales_data.drop('index', axis=1, inplace=True)\n",
    "sales_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze outliers and determine whether to retain or remove them based on their impact on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_data['Customer_ID'] = sales_data['Customer_ID'].astype('')\n",
    "sales_data['Age'] = sales_data['Age'].astype('int')\n",
    "# sales_data['Gender'] = sales_data['Gender'].astype('')\n",
    "# sales_data['Income_Level'] = sales_data['Income_Level'].astype('')\n",
    "# sales_data['Address'] = sales_data['Address'].astype('')\n",
    "# sales_data['Transaction_ID'] = sales_data['Transaction_ID'].astype('')\n",
    "sales_data['Purchase_Date'] = sales_data['Purchase_Date'].astype('datetime64[ns]')\n",
    "# sales_data['Product_ID'] = sales_data['Product_ID'].astype('')\n",
    "# sales_data['Product_Category'] = sales_data['Product_Category'].astype('')\n",
    "# sales_data['Brand'] = sales_data['Brand'].astype('')\n",
    "sales_data['Purchase_Amount'] = sales_data['Purchase_Amount'].astype('int')\n",
    "sales_data['Average_Spending_Per_Purchase'] = sales_data['Average_Spending_Per_Purchase'].astype('int')\n",
    "sales_data['Purchase_Frequency_Per_Month'] = sales_data['Purchase_Frequency_Per_Month'].astype('int')\n",
    "sales_data['Brand_Affinity_Score'] = sales_data['Brand_Affinity_Score'].astype('int')\n",
    "# sales_data['Product_Category_Preferences'] = sales_data['Product_Category_Preferences'].astype('')\n",
    "sales_data['Month'] = sales_data['Month'].astype('int')\n",
    "sales_data['Year'] = sales_data['Year'].astype('int')\n",
    "# sales_data['Season'] = sales_data['Season'].astype('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = sales_data.describe()\n",
    "IQR = description.loc['75%'] - description.loc['25%']\n",
    "integer_columns = sales_data.select_dtypes(include='int').columns\n",
    "for column in integer_columns:\n",
    "    lower_bound = description.loc['25%', column] - 1.5 * IQR[column]\n",
    "    upper_bound = description.loc['75%', column] + 1.5 * IQR[column]\n",
    "    filtered_values = sales_data[~((sales_data[column] >= lower_bound) & (sales_data[column] <= upper_bound))][column]\n",
    "    print(\"Values between {} and {}: {}\".format(lower_bound, upper_bound, filtered_values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that no outlier present here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Address inconsistencies in data format and encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the inconsistencies in the data format and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all is in objects, let's convert them to their respective format for proper precessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_data['Customer_ID'] = sales_data['Customer_ID'].astype('')\n",
    "sales_data['Age'] = sales_data['Age'].astype('int')\n",
    "# sales_data['Gender'] = sales_data['Gender'].astype('')\n",
    "# sales_data['Income_Level'] = sales_data['Income_Level'].astype('')\n",
    "# sales_data['Address'] = sales_data['Address'].astype('')\n",
    "# sales_data['Transaction_ID'] = sales_data['Transaction_ID'].astype('')\n",
    "sales_data['Purchase_Date'] = sales_data['Purchase_Date'].astype('datetime64[ns]')\n",
    "# sales_data['Product_ID'] = sales_data['Product_ID'].astype('')\n",
    "# sales_data['Product_Category'] = sales_data['Product_Category'].astype('')\n",
    "# sales_data['Brand'] = sales_data['Brand'].astype('')\n",
    "sales_data['Purchase_Amount'] = sales_data['Purchase_Amount'].astype('int')\n",
    "sales_data['Average_Spending_Per_Purchase'] = sales_data['Average_Spending_Per_Purchase'].astype('int')\n",
    "sales_data['Purchase_Frequency_Per_Month'] = sales_data['Purchase_Frequency_Per_Month'].astype('int')\n",
    "sales_data['Brand_Affinity_Score'] = sales_data['Brand_Affinity_Score'].astype('int')\n",
    "# sales_data['Product_Category_Preferences'] = sales_data['Product_Category_Preferences'].astype('')\n",
    "sales_data['Month'] = sales_data['Month'].astype('int')\n",
    "sales_data['Year'] = sales_data['Year'].astype('int')\n",
    "# sales_data['Season'] = sales_data['Season'].astype('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new features that provide deeper insights into customer behavior, such as:\n",
    "* Average spending per purchase\n",
    "* Purchase frequency per month\n",
    "* Brand affinity score (based on product brand preferences)\n",
    "* Product category preferences (e.g., TVs, smartphones, laptops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that all of these are already present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize or normalize numeric features to ensure they contribute equally to the clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's standerdize the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_columns = sales_data.select_dtypes(include='int')\n",
    "for column in integer_columns:\n",
    "    sales_data[column+\"_n\"] = (sales_data[column] - sales_data[column].mean()) / sales_data[column].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Exploratory Data Analysis(EDA):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Univariate Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the distribution of key features like customer age, purchase amount, and purchase frequency using histograms, boxplots, and descriptive statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the key features using histogram, boxplots and descriptive statistice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns with their datatype\n",
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data that isn't normalized in another dataframe name eda_df for exploratory data analysis\n",
    "eda_columns = sales_data.select_dtypes(include=['int', 'object', 'datetime64[ns]'])\n",
    "eda_df = sales_data[eda_columns.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical and object type columns seperately\n",
    "display(eda_df.describe())\n",
    "display(eda_df.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_columns = eda_df.select_dtypes(include='int').columns\n",
    "splits = 2\n",
    "integer_columns_length = len(integer_columns)\n",
    "fig = plt.figure(figsize=(12, 36))\n",
    "sns.set(palette='tab10')\n",
    "for index, i in enumerate(range(0, integer_columns_length * 2, 2)):\n",
    "    title = \" \".join(integer_columns[index].split(\"_\"))\n",
    "    temp_axes = fig.add_subplot(math.ceil(integer_columns_length*2/splits), splits, i+1)\n",
    "    temp_axes.hist(eda_df[integer_columns[index]], color='r')\n",
    "    temp_axes.grid(axis='x')\n",
    "    temp_axes.set_title(title + \" among number of sales\")\n",
    "    temp_axes.set_ylabel('Number of sales')\n",
    "    temp_axes.set_xlabel(title)\n",
    "\n",
    "    temp_axes1 = fig.add_subplot(math.ceil(integer_columns_length*2/splits), splits, i+2)\n",
    "    temp_axes1 = sns.boxplot(y=eda_df[integer_columns[index]], color='g')\n",
    "    temp_axes1.grid(axis='x')\n",
    "    temp_axes1.set_title(title + \" boxplot\")\n",
    "    temp_axes1.set_ylabel(title)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_columns = [\n",
    "    'Season','Gender', 'Income_Level', 'Product_Category', 'Brand', 'Purchase_Frequency_Per_Month', \n",
    "    'Brand_Affinity_Score', 'Product_Category_Preferences', 'Month'\n",
    "                    ]\n",
    "splits = 1\n",
    "integer_columns_length = len(integer_columns)\n",
    "\n",
    "for i in range(integer_columns_length):\n",
    "    title = \" \".join(integer_columns[i].split(\"_\"))\n",
    "    # temp_axes = fig.add_subplot(math.ceil(integer_columns_length/splits), splits, i+1)\n",
    "    data = eda_df[integer_columns[i]].value_counts()\n",
    "    plt.figure(figsize=(12, 40))\n",
    "    plt.pie(data, autopct='%.0f%%', colors=sns.color_palette('pastel'), labels=data.index)\n",
    "    # temp_axes.grid(axis='x')\n",
    "    plt.title(title + \" pie chart\")\n",
    "    # temp_axes.ylabel(title)\n",
    "    # break\n",
    "    # plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    title = \" \".join(integer_columns[i].split(\"_\"))\n",
    "    # temp_axes = fig.add_subplot(math.ceil(integer_columns_length/splits), splits, i+1)\n",
    "    sns.barplot(eda_df[integer_columns[i]].value_counts(), color=['r', 'b', 'g', 'y'][i%4])\n",
    "    temp_axes.grid(axis='x')\n",
    "    temp_axes.set_title(title + \" barplot\")\n",
    "    temp_axes.set_ylabel(title)\n",
    "    plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer_columns = [\n",
    "#     'Season','Gender', 'Income_Level', 'Product_Category', 'Brand', 'Purchase_Frequency_Per_Month', \n",
    "#     'Brand_Affinity_Score', 'Product_Category_Preferences', 'Month'\n",
    "#                     ]\n",
    "# splits = 1\n",
    "# integer_columns_length = len(integer_columns)\n",
    "# fig = plt.figure(figsize=(12, 40))\n",
    "# for i in range(integer_columns_length):\n",
    "#     title = \" \".join(integer_columns[i].split(\"_\"))\n",
    "#     temp_axes = fig.add_subplot(math.ceil(integer_columns_length/splits), splits, i+1)\n",
    "#     temp_axes = sns.barplot(eda_df[integer_columns[i]].value_counts(), color=['r', 'b', 'g', 'y'][i%4])\n",
    "#     temp_axes.grid(axis='x')\n",
    "#     temp_axes.set_title(title + \" barplot\")\n",
    "#     temp_axes.set_ylabel(title)\n",
    "#     # break\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify potential skewness or outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On seeing the data, we can see that no outlier is present in data and also data is not too much skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bivariate Analysis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize scatterplots and heatmaps to explore relationships between different features, such as purchase amount vs. income level, brand affinity vs. product category, and purchase frequency vs. age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_columns = eda_df.select_dtypes(include='int').columns\n",
    "splits = 1\n",
    "temp = 0\n",
    "integer_columns_length = len(integer_columns)\n",
    "fig = plt.figure(figsize=(12, 200))\n",
    "for i in range(integer_columns_length):\n",
    "    for j in range(integer_columns_length):\n",
    "        if i!= j:\n",
    "            temp = temp + 1\n",
    "            title_j = \" \".join(integer_columns[j].split(\"_\"))\n",
    "            title_i = \" \".join(integer_columns[i].split(\"_\"))\n",
    "            temp_axes = fig.add_subplot(math.ceil((integer_columns_length*integer_columns_length - integer_columns_length)/splits), splits, temp)\n",
    "            temp_axes = sns.regplot(x=eda_df[integer_columns[i]], y=eda_df[integer_columns[j]], \n",
    "                                    scatter_kws={\"color\": \"g\"}, line_kws={\"color\": \"r\"})\n",
    "            temp_axes.grid(axis='x')\n",
    "            temp_axes.set_title(title_i + \" among \" + title_j)\n",
    "            temp_axes.set_ylabel(title_j)\n",
    "            temp_axes.set_xlabel(title_i)\n",
    "            # break\n",
    "    # break\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = eda_df.select_dtypes(include=['int', 'datetime64[ns]']).corr()\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.heatmap(correlation, cmap=\"viridis\", fmt=\".2f\", annot=True)\n",
    "ax.set_xticks(np.arange(correlation.shape[1])+0.5)\n",
    "ax.set_yticks(np.arange(correlation.shape[0])+0.5)\n",
    "ax.set_xticklabels(correlation.columns)\n",
    "ax.set_yticklabels(correlation.index)\n",
    "ax.tick_params(axis='x', rotation = 90)\n",
    "# fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the presence of correlations and identify any impactful relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that very low correlation is present between fields and also the linear line shows very less dependency of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_columns = ['Purchase_Amount', 'Average_Spending_Per_Purchase', 'Purchase_Frequency_Per_Month']\n",
    "splits = 1\n",
    "integer_columns_length = len(integer_columns)\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "group_by_data = eda_df.groupby(eda_df['Purchase_Date'])\n",
    "for i in range(integer_columns_length):\n",
    "        title_i = \" \".join(integer_columns[i].split(\"_\"))\n",
    "        temp_axes = fig.add_subplot(math.ceil(integer_columns_length/splits), splits, i + 1)\n",
    "        data = group_by_data[integer_columns[i]].sum()\n",
    "        # cum_data = group_by_data[integer_columns[i]].count()\n",
    "        # for temp in data:\n",
    "        # display(data)\n",
    "        # display(data.index)\n",
    "        # data.unstack()\n",
    "        # data.plot(kind=\"line\")\n",
    "        # temp_axes = eda_df['Purchase_Date'].dt.year.value_counts().plot(kind=\"line\")\n",
    "        temp_axes.plot(data.index, data.values)\n",
    "        print(data)\n",
    "        # temp_axes.plot(cum_data.index, cum_data.values)\n",
    "        # temp_axes.grid(axis='x')\n",
    "        temp_axes.set_title(title_i + \" trend over time\")\n",
    "        # temp_axes.set_xlabel(\"Purchase Date\")\n",
    "        # temp_axes.set_ylabel(title_i)\n",
    "        # if i >= 0:\n",
    "        # break\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age_n VS Average_Spending_Per_Purchase_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales_data[['Age_n', 'Average_Spending_Per_Purchase_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_scaled)\n",
    "    silhouette_avg = silhouette_score(data_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Analysis for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimal_k = 6\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "sales_data['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', hue='Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Spending (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age_n VS Purchase_Frequency_Per_Month_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales_data[['Age_n', 'Purchase_Frequency_Per_Month_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_scaled)\n",
    "    silhouette_avg = silhouette_score(data_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Analysis for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimal_k = 7\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "sales_data['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age_n', y='Purchase_Frequency_Per_Month_n', hue='Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Purchase_Frequency_Per_Month (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age_n VS Purchase_Amount_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales_data[['Age_n', 'Purchase_Amount_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_scaled)\n",
    "    silhouette_avg = silhouette_score(data_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Analysis for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimal_k = 10\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "sales_data['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age_n', y='Purchase_Amount_n', hue='Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Purchase_Frequency_Per_Month (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchase_Amount_n VS Purchase_Frequency_Per_Month_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales_data[['Purchase_Amount_n', 'Purchase_Frequency_Per_Month_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_scaled)\n",
    "    silhouette_avg = silhouette_score(data_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Analysis for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimal_k = 9  \n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "sales_data['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Purchase_Amount_n', y='Purchase_Frequency_Per_Month_n', hue='Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Purchase_Frequency_Per_Month (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchase Amount VS Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define eps and MinPts parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "data = sales_data[['Age_n', 'Purchase_Amount_n']]\n",
    "\n",
    "eps_values = [0.1, 0.5, 1.0, 1.5]\n",
    "min_samples_values = [3, 5, 7]\n",
    "\n",
    "best_silhouette_score = -1\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "print(\"All Combinations:\")\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(data)\n",
    "\n",
    "        unique_labels = np.unique(labels)\n",
    "\n",
    "        if len(unique_labels) == 1:\n",
    "            continue\n",
    "\n",
    "        silhouette_avg = silhouette_score(data, labels)\n",
    "        print(f\"For eps={eps}, min_samples={min_samples}, Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(f\"eps={best_eps}, min_samples={best_min_samples}, Silhouette Score: {best_silhouette_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales_data[['Age_n', 'Purchase_Amount_n']]\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5) \n",
    "sales_data['Cluster'] = dbscan.fit_predict(data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "core_points_mask = dbscan.core_sample_indices_\n",
    "sns.scatterplot(x='Age_n', y='Purchase_Amount_n', hue='Cluster', data=sales_data,\n",
    "                palette={-1: 'gray', 0: 'red', 1: 'blue', 2: 'green', 3: 'purple'},  \n",
    "                legend='full', style=sales_data['Cluster'], markers=[\"o\"], s=50)\n",
    "\n",
    "border_points_mask = ~core_points_mask & (sales_data['Cluster'] != -1)\n",
    "sns.scatterplot(x='Age_n', y='Purchase_Amount_n', data=sales_data[border_points_mask],\n",
    "                color='blue', marker='o', label='Border point', s=50)\n",
    "\n",
    "noise_points_mask = (sales_data['Cluster'] == -1)\n",
    "sns.scatterplot(x='Age_n', y='Purchase_Amount_n', data=sales_data[noise_points_mask],\n",
    "                color='gray', marker='X', label='Noise', s=50)\n",
    "\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Age_n')\n",
    "plt.ylabel('Purchase_Amount_n (Normalized)')\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "labels = [label.replace('0', 'Core Point') for label in labels]  \n",
    "plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchase_Amount_n VS Purchase_Frequency_Per_Month_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define eps and MinPts parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "data = sales_data[['Purchase_Amount_n', 'Purchase_Frequency_Per_Month_n']]\n",
    "\n",
    "eps_values = [0.1, 0.5, 1.0, 1.5]\n",
    "min_samples_values = [3, 5, 7]\n",
    "\n",
    "best_silhouette_score = -1\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "print(\"All Combinations:\")\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(data)\n",
    "\n",
    "        unique_labels = np.unique(labels)\n",
    "\n",
    "        if len(unique_labels) == 1:\n",
    "            continue\n",
    "\n",
    "        silhouette_avg = silhouette_score(data, labels)\n",
    "        print(f\"For eps={eps}, min_samples={min_samples}, Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(f\"eps={best_eps}, min_samples={best_min_samples}, Silhouette Score: {best_silhouette_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = sales_data[['Purchase_Amount_n', 'Purchase_Frequency_Per_Month_n']]\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  \n",
    "sales_data['Cluster'] = dbscan.fit_predict(data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "core_points_mask = dbscan.core_sample_indices_\n",
    "sns.scatterplot(x='Purchase_Amount_n', y='Purchase_Frequency_Per_Month_n', hue='Cluster', data=sales_data,\n",
    "                palette={-1: 'gray', 0: 'red', 1: 'blue', 2: 'green', 3: 'purple'},  \n",
    "                legend='full', style=sales_data['Cluster'], markers=[\"o\"], s=50)\n",
    "\n",
    "border_points_mask = ~core_points_mask & (sales_data['Cluster'] != -1)\n",
    "sns.scatterplot(x='Purchase_Amount_n', y='Purchase_Frequency_Per_Month_n', data=sales_data[border_points_mask],\n",
    "                color='blue', marker='o', label='Border point', s=50)\n",
    "\n",
    "noise_points_mask = (sales_data['Cluster'] == -1)\n",
    "sns.scatterplot(x='Purchase_Amount_n', y='Purchase_Frequency_Per_Month_n', data=sales_data[noise_points_mask],\n",
    "                color='gray', marker='X', label='Noise', s=50)\n",
    "\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Purchase_Amount')\n",
    "plt.ylabel('Purchase_Frequency_Per_Month (Normalized)')\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "labels = [label.replace('0', 'Core Point') for label in labels]  \n",
    "plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age_n VS Average_Spending_Per_Purchase_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define eps and MinPts parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "data = sales_data[['Age_n', 'Average_Spending_Per_Purchase_n']]\n",
    "\n",
    "eps_values = [0.1, 0.5, 1.0, 1.5]\n",
    "min_samples_values = [3, 5, 7]\n",
    "\n",
    "best_silhouette_score = -1\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "print(\"All Combinations:\")\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(data)\n",
    "\n",
    "        unique_labels = np.unique(labels)\n",
    "\n",
    "        if len(unique_labels) == 1:\n",
    "            continue\n",
    "\n",
    "        silhouette_avg = silhouette_score(data, labels)\n",
    "        print(f\"For eps={eps}, min_samples={min_samples}, Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(f\"eps={best_eps}, min_samples={best_min_samples}, Silhouette Score: {best_silhouette_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = sales_data[['Age_n', 'Average_Spending_Per_Purchase_n']]\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  \n",
    "sales_data['Cluster'] = dbscan.fit_predict(data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "core_points_mask = dbscan.core_sample_indices_\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', hue='Cluster', data=sales_data,\n",
    "                palette={-1: 'gray', 0: 'red', 1: 'blue', 2: 'green', 3: 'purple'},  \n",
    "                legend='full', style=sales_data['Cluster'], markers=[\"o\"], s=50)\n",
    "\n",
    "border_points_mask = ~core_points_mask & (sales_data['Cluster'] != -1)\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', data=sales_data[border_points_mask],\n",
    "                color='blue', marker='o', label='Border point', s=50)\n",
    "\n",
    "noise_points_mask = (sales_data['Cluster'] == -1)\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', data=sales_data[noise_points_mask],\n",
    "                color='gray', marker='X', label='Noise', s=50)\n",
    "\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Spending (Normalized)')\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "labels = [label.replace('0', 'Core Point') for label in labels] \n",
    "plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute DBSCAN VS K Means for Age_n for Average_Spending_Per_Purchase_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = sales_data[['Age_n', 'Average_Spending_Per_Purchase_n']]\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "sales_data['DBSCAN_Cluster'] = dbscan.fit_predict(data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "core_points_mask = dbscan.core_sample_indices_\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', hue='DBSCAN_Cluster', data=sales_data,\n",
    "                palette={-1: 'gray', 0: 'red', 1: 'blue', 2: 'green', 3: 'purple'},\n",
    "                legend='full', style=sales_data['DBSCAN_Cluster'], markers=[\"o\"], s=50)\n",
    "\n",
    "border_points_mask = ~core_points_mask & (sales_data['DBSCAN_Cluster'] != -1)\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', data=sales_data[border_points_mask],\n",
    "                color='blue', marker='o', label='Border point', s=50)\n",
    "\n",
    "noise_points_mask = (sales_data['DBSCAN_Cluster'] == -1)\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', data=sales_data[noise_points_mask],\n",
    "                color='gray', marker='X', label='Noise', s=50)\n",
    "\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Spending (Normalized)')\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "labels = [label.replace('0', 'Core Point') for label in labels]  \n",
    "plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = sales_data[['Age_n', 'Average_Spending_Per_Purchase_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "optimal_k = 4  \n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "sales_data['KMeans_Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', hue='KMeans_Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Spending (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales_data[['Age_n', 'Average_Spending_Per_Purchase_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, init='k-means++')  # Use K-means++ initialization\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "optimal_k = 5 \n",
    "\n",
    "\n",
    "kmeans_initial = KMeans(n_clusters=1, init='k-means++', random_state=42)\n",
    "kmeans_initial.fit(data_scaled)\n",
    "initial_centroid = kmeans_initial.cluster_centers_\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k - 1, init='k-means++', random_state=42)\n",
    "kmeans.fit(data_scaled)\n",
    "remaining_centroids = kmeans.cluster_centers_\n",
    "\n",
    "initial_centroids = np.concatenate((initial_centroid, remaining_centroids), axis=0)\n",
    "\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, init=initial_centroids, n_init=1, random_state=42)\n",
    "sales_data['KMeans_Cluster'] = kmeans_final.fit_predict(data_scaled)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', hue='KMeans_Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans_final.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Spending (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales_data[['Age_n', 'Purchase_Frequency_Per_Month_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "optimal_k = 5\n",
    "\n",
    "kmeans_initial = KMeans(n_clusters=1, init='k-means++', random_state=42)\n",
    "kmeans_initial.fit(data_scaled)\n",
    "initial_centroid = kmeans_initial.cluster_centers_\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k - 1, init='k-means++', random_state=42)\n",
    "kmeans.fit(data_scaled)\n",
    "remaining_centroids = kmeans.cluster_centers_\n",
    "\n",
    "initial_centroids = np.concatenate((initial_centroid, remaining_centroids), axis=0)\n",
    "\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, init=initial_centroids, n_init=1, random_state=42)\n",
    "sales_data['Cluster'] = kmeans_final.fit_predict(data_scaled)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age_n', y='Average_Spending_Per_Purchase_n', hue='Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans_final.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids (Custom K-means++ Initialization)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Spending (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sales_data[['Age_n', 'Purchase_Amount_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "optimal_k = 5 \n",
    "\n",
    "kmeans_initial = KMeans(n_clusters=1, init='k-means++', random_state=42)\n",
    "kmeans_initial.fit(data_scaled)\n",
    "initial_centroid = kmeans_initial.cluster_centers_\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k - 1, init='k-means++', random_state=42)\n",
    "kmeans.fit(data_scaled)\n",
    "remaining_centroids = kmeans.cluster_centers_\n",
    "\n",
    "initial_centroids = np.concatenate((initial_centroid, remaining_centroids), axis=0)\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, init=initial_centroids, n_init=1, random_state=42)\n",
    "sales_data['Cluster'] = kmeans_final.fit_predict(data_scaled)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age_n', y='Purchase_Amount_n', hue='Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans_final.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids (Custom K-means++ Initialization)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Purchase Amount (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = sales_data[['Purchase_Amount_n', 'Purchase_Frequency_Per_Month_n']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "optimal_k = 4\n",
    "\n",
    "\n",
    "kmeans_initial = KMeans(n_clusters=1, init='k-means++', random_state=42)\n",
    "kmeans_initial.fit(data_scaled)\n",
    "initial_centroid = kmeans_initial.cluster_centers_\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k - 1, init='k-means++', random_state=42)\n",
    "kmeans.fit(data_scaled)\n",
    "remaining_centroids = kmeans.cluster_centers_\n",
    "\n",
    "initial_centroids = np.concatenate((initial_centroid, remaining_centroids), axis=0)\n",
    "\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, init=initial_centroids, n_init=1, random_state=42)\n",
    "sales_data['Cluster'] = kmeans_final.fit_predict(data_scaled)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Purchase_Amount_n', y='Purchase_Frequency_Per_Month_n', hue='Cluster', data=sales_data, palette='viridis', legend='full')\n",
    "\n",
    "centroids = kmeans_final.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='o', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering with Centroids (Custom K-means++ Initialization)')\n",
    "plt.xlabel('Purchase Amount (Normalized)')\n",
    "plt.ylabel('Purchase Frequency Per Month (Normalized)')\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of all three clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "Uses the concept of centroids to form clusters\n",
    "Requires the number of clusters K to be specified in advance.\n",
    "\n",
    "\n",
    "## K-Means++\n",
    "\n",
    "A variation of the K-Means algorithm that improves the selection of initial centroids.\n",
    "Selects initial centroids in a way that minimizes the variance between clusters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## DBSCAN\n",
    "DBScan is useful for datasets with arbitrary shapes, varying densities, and when the number of clusters is not known beforehand. K-Means++ can be used as an improvement over K-Means to achieve more stable clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = sales_data[['Purchase_Amount_n', 'Purchase_Frequency_Per_Month_n']]\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "silhouette_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "\n",
    "k_values = range(2, 11)\n",
    "\n",
    "\n",
    "for k in k_values:\n",
    "    # K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(data_scaled)\n",
    "    silhouette_scores.append(silhouette_score(data_scaled, kmeans_labels))\n",
    "    calinski_harabasz_scores.append(calinski_harabasz_score(data_scaled, kmeans_labels))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(data_scaled, kmeans_labels))\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Silhouette Score\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "# Calinski-Harabasz Score\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(k_values, calinski_harabasz_scores, marker='o')\n",
    "plt.title('Calinski-Harabasz Score')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(k_values, davies_bouldin_scores, marker='o')\n",
    "plt.title('Davies-Bouldin Index')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "Advantages\n",
    "\n",
    "K-Means might be a good choice if the number of customer segments is known or can be estimated reliably\n",
    "\n",
    "Simple and easy to implement.\n",
    "\n",
    "Computationally efficient for large datasets.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "Requires the number of clusters K to be specified in advance, which may not always be known.\n",
    "\n",
    "Sensitive to initial centroid selection and can converge to a local minimum.\n",
    "\n",
    "## DBSCAN:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "DBSCAN could be useful if the data has varying cluster densities or if there are outliers that need to be identified as noise points\n",
    "\n",
    "Does not require the number of clusters to be specified in advance, making it suitable for datasets with varying cluster densities\n",
    "\n",
    "Can identify noise points as outliers and does not assume spherical clusters.\n",
    "\n",
    "Robust to outliers and able to handle irregularly shaped clusters.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "Sensitivity to the distance metric used and the choice of epsilon (eps) and min_samples parameters.\n",
    "\n",
    "Can be computationally expensive for large datasets, especially when density is not well-defined.\n",
    "\n",
    "\n",
    "\n",
    "## K-Means++\n",
    "\n",
    "Advantages\n",
    "\n",
    "K-Means++ could improve the performance of K-Means by providing better initial centroids, potentially leading to more stable and accurate clusters.\n",
    "\n",
    "Improves K-Means' convergence and reduces the likelihood of converging to a local minimum by initializing centroids in a more \n",
    "strategic manner.\n",
    "Can result in more stable and accurate cluster assignments compared to traditional K-Means.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "Adds some computational overhead during initialization due to the extra steps involved in selecting initial centroids.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw conclusions and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Centroids \n",
    "K-means++ selects initial cluster centroids that are spread out and well-distributed across the data, which helps in finding better cluster centers.provide a  better overall clustering solution compared to the random initialization used in standard K-means.\n",
    "\n",
    "## Improved Convergence\n",
    "By starting with better initial centroids, K-means++ often converges faster than the standard K-means algorithm. This is because the initial centroids are already closer to their final positions, reducing the number of iterations needed to reach convergence.\n",
    "\n",
    "## Reduced Sensitivity to Initialization\n",
    "Standard K-means can be sensitive to the initial placement of centroids but  K-means++ reduces this sensitivity by providing a more systematic initialization strategy, resulting in more consistent and reliable clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Analysis \n",
    "\n",
    "## Part A\n",
    "\n",
    "K-Means and DBSCAN are two methods for sorting customers into groups and making clusters based on their shopping from the  store\n",
    "\n",
    "K-Means is equal-sized clusters but may not work well for all types of data\n",
    "\n",
    "DBSCAN is better at finding arbitrary-shaped clusters \n",
    "\n",
    "## Part B\n",
    "\n",
    "The study found three main groups of shoppers at Imtiaz Mall\n",
    "\n",
    "High-Spending Customers They love expensive brands and buy new things often\n",
    "\n",
    "Moderate-Spending Customers  They spend a steady amount of money and care about quality\n",
    "\n",
    "Budget-Conscious Customers They look for good deals and do not care as much about brands\n",
    "\n",
    "## Part C\n",
    "\n",
    "Imtiaz Mall can use this information to\n",
    "\n",
    "Create ads and publish on social media\n",
    "\n",
    "Send Promotional Emails to its customers\n",
    "\n",
    "Custom  product Recommendation\n",
    "\n",
    "## Part D\n",
    "\n",
    "Retain Customers\n",
    "\n",
    "Increae Sale\n",
    "\n",
    "Create social media Campigns\n",
    "\n",
    "Understand Customer Requirements \n",
    "\n",
    "Track how each groups shopping habits change over time\n",
    "\n",
    "Predict Future Sales of Store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
